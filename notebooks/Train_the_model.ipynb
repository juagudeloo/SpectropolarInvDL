{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8aa388a-3153-4b7a-b0e0-caff0afb2bb9",
   "metadata": {},
   "source": [
    "# Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8b6b3-4b1f-45d0-8c14-1fe3574251c3",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c81c8-68c1-4115-9bd3-acd2ce85061e",
   "metadata": {},
   "source": [
    "First, we will import the necessary libraries from python to define our neural network models and other small processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac13c9e-e7f8-4dd0-a838-d3c4112432a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from timeit import default_timer as timer \n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78eef3-e7f1-4821-af29-db9dd0344c69",
   "metadata": {},
   "source": [
    "Second, we will import a module that defines a class which will help us to upload the atmosphere quantities and the stokes parameters just as it was done in the notebook about [Charging the data](./Charging_the_data.ipynb])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5051baa-cf90-4248-8e07-5730f629d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../modules\")\n",
    "from ChargeData import MURaM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45583ce9-38c7-4f9e-b483-6652c952cbd6",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170fb9c-3c30-4200-8a12-bc079fcb4fd2",
   "metadata": {},
   "source": [
    "### 3. Charge the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b03afd-a8ba-42b7-8a34-3385e068a301",
   "metadata": {},
   "source": [
    "Using the `MURaM` class we will charge the granular-intergranular leveraged data for various filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da7199-ec2d-42f7-a2bc-7394b1b484cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames to be readed for creating the dataset\n",
    "filenames = [\"080000\", \"085000\", \"090000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf8709-cc12-454c-82d2-1468cda32675",
   "metadata": {},
   "source": [
    "Let's concatenate all the files data to create a unified dataset of atmophere magnitudes with their corresponding stokes parameters spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e3194-cbf1-452c-8b62-e5872066d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrays for saving the whole dataset\n",
    "atm_data = []\n",
    "stokes_data = []\n",
    "\n",
    "for fln in filenames:\n",
    "    #Creation of the MURaM object for each filename for charging the data.\n",
    "    muram = MURaM(filename=fln)\n",
    "    muram.charge_quantities()\n",
    "    muram.optical_depth_stratification()\n",
    "    muram.degrade_spec_resol()\n",
    "    muram.scale_quantities()\n",
    "    muram.gran_intergran_balance()\n",
    "\n",
    "    atm_data.append(muram.atm_quant)\n",
    "    stokes_data.append(muram.stokes)\n",
    "\n",
    "atm_data = np.concatenate(atm_data, axis=0)\n",
    "stokes_data = np.concatenate(stokes_data, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abf979b-fb00-4967-89e4-0853959e4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_data.shape, stokes_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9df2e-858f-43bc-a89d-2f7f7f322c65",
   "metadata": {},
   "source": [
    "### 2.2 Training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b638f0b-73d8-4994-94e5-9149265b5c2a",
   "metadata": {},
   "source": [
    "Once we have the data uploaded, we may need to separate it between training and testing sets for applying the data set to the neural network learning process. The train set will have a 70% size of the whole dataset while the test set will have a 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafec501-61bb-4bb2-80ae-d5d88dacadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train, in_test, out_train, out_test = train_test_split(stokes_data, atm_data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff55bd-6e03-4e0d-be4a-d3b0cbda9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"in_train shape:\", in_train.shape)\n",
    "print(\"out_train shape:\", out_train.shape)\n",
    "print(\"in_test shape:\", in_test.shape)\n",
    "print(\"out_test shape:\", out_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6a294-e90e-46f5-8e6c-1623d189eadb",
   "metadata": {},
   "source": [
    "Having the test and train sets defined, let's charge them inside pytorch dataloaders for the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f78fb3-2568-45a4-92f1-b9c86c838d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Tensors stored in: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a70b96-3dd5-4fe8-9708-3da20c40f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the test and train datasets to tensors\n",
    "in_train = torch.from_numpy(in_train).to(device)\n",
    "in_test = torch.from_numpy(in_test).to(device)\n",
    "out_train = torch.from_numpy(out_train).to(device)\n",
    "out_test = torch.from_numpy(out_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e0d08-6c16-4ed8-993a-e65ada56e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"in_train shape:\", in_train.size())\n",
    "print(\"out_train shape:\", out_train.size())\n",
    "print(\"in_test shape:\", in_test.size())\n",
    "print(\"out_test shape:\", out_test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1841d42-ffc5-41ac-9ccf-a773dcc059a4",
   "metadata": {},
   "source": [
    "Because the output of the neural network model we are going to use is linear, it is necessary to flatten the last two axis of the out datasets corresponding to the atmosphere magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b14b6-7d66-4ab3-8cbb-36eb22cf1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening of the output external axes\n",
    "out_train = torch.reshape(out_train, (out_train.size()[0], out_train.size()[1]*out_train.size()[2]))\n",
    "out_test = torch.reshape(out_test, (out_test.size()[0], out_test.size()[1]*out_test.size()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12346add-a58b-4368-be48-00cdeb35440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"in_train shape:\", in_train.size())\n",
    "print(\"out_train shape:\", out_train.size())\n",
    "print(\"in_test shape:\", in_test.size())\n",
    "print(\"out_test shape:\", out_test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d8f2a-78b4-40cf-93dc-ba6fb7ab1029",
   "metadata": {},
   "source": [
    "Finally, having both datasets converted to tensors, let's save them in their corresponding pytorch dataloaders. Here we will define the batch size hyperparameter for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b050f4-9967-4f60-8279-96f1844f2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "batch_size = 80\n",
    "\n",
    "#Train and test datasets\n",
    "train_dataset = TensorDataset(in_train.to(device), out_train.to(device))\n",
    "test_dataset = TensorDataset(in_test.to(device), out_test.to(device))\n",
    "\n",
    "#Train and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "        batch_size=batch_size, # how many samples per batch? \n",
    "        shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")    \n",
    "\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {batch_size}\")\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(f\"\"\"\n",
    "Shape of each batch input and output:\n",
    "train input batch shape: {train_features_batch.shape}, \n",
    "train output batch shape: {train_labels_batch.shape}\n",
    "        \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f9edc-afe2-46af-b5ba-d09c86026c87",
   "metadata": {},
   "source": [
    "## 3. Neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddae808-7f84-4ba2-a968-8bd72206919f",
   "metadata": {},
   "source": [
    "Now that we have the datasets defined, let's create our neural network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b397f32-2643-41eb-a7f4-17cb6dd78701",
   "metadata": {},
   "source": [
    "### 3.1 Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256bfde-a233-4887-95d8-74e465c044b7",
   "metadata": {},
   "source": [
    "First let's use a very simple convolutional 1D neural network arquitecture with a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7892891-bbde-499e-8a57-feff6c147039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, hidden_units):\n",
    "        super().__init__()\n",
    "        padding = 1\n",
    "        self.simple_conv = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=in_shape, out_channels=72, kernel_size = 2, stride=1, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(p=0.5, inplace=False),\n",
    "        nn.Linear(in_features = 360, out_features = out_shape)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.simple_conv(x)\n",
    "\n",
    "simple_model = SimpleModel(36,6*20,hidden_units=4096).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266def2-5642-4eda-b22a-9c2ab91f9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2eeb7-c940-454b-b176-6c923eb63367",
   "metadata": {},
   "source": [
    "It's important that the datasets and the model are both in the same device for performind the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fd618-d2f5-44eb-9177-682854462a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThe model will be runned in:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248a916-6612-4183-9304-9cf859e03e48",
   "metadata": {},
   "source": [
    "Define the model loss function and optimizer, along with the learning rate and epochs hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058331c-4788-41b7-aff3-03fcc314e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs\n",
    "epochs = 20\n",
    "\n",
    "#Learning rate\n",
    "lr = 1e-5\n",
    "\n",
    "#Loss function\n",
    "loss_fn = nn.MSELoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69051b4-0571-41f6-91dd-757022b60fd8",
   "metadata": {},
   "source": [
    "We must write down the path for the model to save the results of the training along with the trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18057d00-e578-4787-81c6-f589a236a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_out = \"Results/\"\n",
    "if not os.path.exists(results_out):\n",
    "    os.mkdir(results_out)\n",
    "    \n",
    "#Training folder for the specific hyperparameters\n",
    "pth_out = results_out+f\"{epochs}E_\"+f\"{lr}lr/\"\n",
    "if not os.path.exists(pth_out):\n",
    "    os.mkdir(pth_out)\n",
    "#Create model save path\n",
    "MODEL_PATH = Path(pth_out+\"model_weights/\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME = \"inversion_\"+str(epochs)+\"E\"+str(lr)+\"lr\"+\".pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "#Charge the weights in case there have been some training before\n",
    "if MODEL_SAVE_PATH.exists():\n",
    "    model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f619b-1e62-4b80-9308-5971821f97d8",
   "metadata": {},
   "source": [
    "### 3.2 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba40859-3fde-4689-b215-1aadeb5ee628",
   "metadata": {},
   "source": [
    "We have everything settled! Let's code the training process. First let's define functions for the training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0854f-8c09-4772-9151-bb5c04f67f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module, train_dataloader: DataLoader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Function that performs the training step over all the batches in the train dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model(nn.Module): Model to be trained.\n",
    "        train_dataloader(torch.utils.data.Dataloader): Dataloader of the training dataset.\n",
    "        loss_fn: Loss function for the training process.\n",
    "        optimizer: Optimizer function for the training process.\n",
    "        device: Agnostic device defined por allocating the data and the model.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Add a loop to loop through training batches\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model.train() \n",
    "        # 1. Forward pass\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model.double()(X.double())\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred.double(), y.double())\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def test_step(model: nn.Module, test_dataloader: DataLoader, loss_fn):\n",
    "    \"\"\"\n",
    "    Function that performs the testing step over the testin dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model(nn.Module): Model to be tested.\n",
    "        test_dataloader(torch.utils.data.Dataloader): Dataloader of the testing dataset.\n",
    "        loss_fn: Loss function for the testing the model results.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "        \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "      \n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                test_loss: [...],\n",
    "      For example if training for epochs=2: \n",
    "              {train_loss: [2.0616, 1.0537],\n",
    "                test_loss: [1.2641, 1.5706],\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"test_loss\": [],\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "\n",
    "        ### New: Experiment tracking ###\n",
    "        # Add loss results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"Loss\", \n",
    "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                            \"test_loss\": test_loss},\n",
    "                           global_step=epoch)\n",
    "        \n",
    "        # Track the PyTorch model architecture\n",
    "        writer.add_graph(model=model, \n",
    "                         # Pass in an example input\n",
    "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    \n",
    "    ### End new ###\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results\n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b099f80-5a2d-4536-bdff-e422e7368b34",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71b3a7-bf02-49c7-b394-22fb6597029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = np.zeros((epochs,))\n",
    "test_loss_history = np.zeros((epochs,))\n",
    "\n",
    "total_train_time_model = 0\n",
    "# Set timers\n",
    "train_time_start_on_cpu = timer()\n",
    "start = time.time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "    train_loss_history[epoch] = train_loss\n",
    "    \n",
    "    ### Testing)\n",
    "    test_loss = test_step(model, test_dataloader, loss_fn)\n",
    "    test_loss_history[epoch] = test_loss\n",
    "    \n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")    \n",
    "\n",
    "    # Calculate training time      \n",
    "    train_time_end_on_cpu = timer()\n",
    "    total_train_time_model += print_train_time(start=train_time_start_on_cpu, \n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device=str(next(model.parameters()).device))\n",
    "\n",
    "    # the model state dict after training\n",
    "    print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "    torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "            f=MODEL_SAVE_PATH)\n",
    "    \n",
    "metrics_out = pth_out+\"loss_metrics/\"\n",
    "if not os.path.exists(metrics_out):\n",
    "    os.mkdir(metrics_out)\n",
    "    \n",
    "train_loss_history_path = metrics_out+\"train_loss_history\"+str(epochs)+\"E\"+str(lr)+\"lr\"+\".npy\"\n",
    "test_loss_history_path = metrics_out+\"test_loss_history\"+str(epochs)+\"E\"+str(lr)+\"lr\"+\".npy\"\n",
    "\n",
    "np.save(train_loss_history_path, train_loss_history)\n",
    "np.save(test_loss_history_path, test_loss_history)\n",
    "\n",
    "runtime = time.time()-start\n",
    "with open(metrics_out+\"runtime.txt\", \"w\") as f:\n",
    "    f.write(str(datetime.timedelta(seconds=runtime)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be41a8-6624-43a7-a841-86df6eae4385",
   "metadata": {},
   "source": [
    "### 3.3 Looking up the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5855f9b-1dc1-41de-ac8d-a4a5acb3808c",
   "metadata": {},
   "source": [
    "Let's check out the metrics on how the model got trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2fb2ef-5a10-4ec7-b2f1-acf64cc5a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(2*5,5))\n",
    "epochs_a = range(epochs)\n",
    "\n",
    "ax[0].plot(epochs_a, train_loss_history)\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].set_xlabel(\"epochs\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Train\")\n",
    "ax[1].plot(epochs_a, test_loss_history)\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_xlabel(\"epochs\")\n",
    "ax[1].set_title(\"Train\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
